{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN0nZwyMGadB"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC2: Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "En esta práctica se implementarán tres modelos de DRL en un mismo entorno, con el objetivo de analizar distintas formas de aprendizaje de un agente y estudiar su rendimiento. El agente será entrenado con los métodos:\n",
    "\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "    <li>REINFORCE with baseline </li>\n",
    " </ol>\n",
    "\n",
    "**Importante: La entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File  →  Download as  →  HTML.**\n",
    "\n",
    "**Es necesario adjuntar en la entrega los ficheros .pth con los diferentes modelos entrenados.**\n",
    "\n",
    "No es necesario adjuntar los ficheros .gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Contexto\n",
    "\n",
    "Uno de los objetivos más actuales del campo de la robótica es conseguir que un robot sea capaz de aprender a realizar una serie de acciones por si sólo, del mismo modo que lo hace un niño pequeño. Esta es, básicamente, una de las principales motivaciones del aprendizaje por refuerzo profundo. Para ello se necesitan sistemas de control eficientes en entornos de alta dimensionalidad como puede ser la inversión en bolsa, conducción de coches autónomos o, incluso, el control de cohetes espaciales. Con esta idea, en esta práctica usaremos un entorno ya predefinido en OpenAI, **Space Invader**.\n",
    "\n",
    "**Space Invader** consiste en un cañón que puede disparar hacia arriba y moverse de izquierda a derecha. El objetivo del juego es destruir a los extraterrestres enemigos, que se acercan cada vez más rápido al jugador a medida que este los elimina, y maximiza la puntuación. En este entorno, la observación es una imagen RGB de la pantalla representada por una matriz de forma (210, 160, 3) cómo se observa a continuación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](videos/random_agent_space_invader.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rango de acciones:\n",
    "* NOOP: 0 ( No operacion).\n",
    "* FIRE: 1 (disparar sin moverse)\n",
    "* RIGHT: 2 (mover a la derecha)\n",
    "* LEFT: 3 (mover a la izquierda)\n",
    "* RIGHTFIRE: 4 (disparar y mover a la derecha)\n",
    "* LEFTFIRE: 5 (disparar y mover a la izquierda)\n",
    "\n",
    "\n",
    "Recompensa: La recompensa devuelta por el entorno está compuesta por un valor en el rango [0, 30]. Dependiendo de la nave alienígena destruida, el agente recibe una puntuación diferente.\n",
    " Nuestra tarea es enseñarle una política que le permita hacer una elección \"buena\" para cada estado.\n",
    "\n",
    "Para más detalles sobre la definición del entorno de Space Invader, se recomienda consultar las web: <href>https://www.gymlibrary.dev/environments/atari/space_invaders/</href> y  <href>https://atariage.com/manual_html_page.php?SoftwareLabelID=460</href>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialización y exploración del entorno (1 pto)\n",
    "**IMPORTANTE**: el entorno Space Invader depende de la instalación de los siguientes paquetes\n",
    "<ul>\n",
    "     <li>gym[atari] en la versión 0.25.0</li>\n",
    "     <li>autorom[accept-rom-license]</li>\n",
    "</ul>\n",
    "\n",
    "Este entorno puede ser ejecutado tanto en local como en Kaggle con la versión de GPU P100 y en Google Colab. Recomiendo utlizar Kaggle para los estudiantes que no dispongan de una GPU en local.\n",
    "\n",
    "Empezaremos cargando las principales librerías necesarias para la práctica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T10:23:07.600452Z",
     "iopub.status.busy": "2022-10-26T10:23:07.599977Z",
     "iopub.status.idle": "2022-10-26T10:23:58.641094Z",
     "shell.execute_reply": "2022-10-26T10:23:58.637388Z",
     "shell.execute_reply.started": "2022-10-26T10:23:07.600419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari]==0.25.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from gym[atari]==0.25.0) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from gym[atari]==0.25.0) (1.23.4)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from gym[atari]==0.25.0) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from gym[atari]==0.25.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from ale-py~=0.7.5->gym[atari]==0.25.0) (5.10.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: click in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from autorom[accept-rom-license]) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from autorom[accept-rom-license]) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from autorom[accept-rom-license]) (2.28.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from autorom[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.4)\n",
      "Requirement already satisfied: imageio in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (2.22.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from imageio) (9.2.0)\n",
      "Requirement already satisfied: numpy in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from imageio) (1.23.4)\n",
      "Requirement already satisfied: matplotlib in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages/AutoROM/roms/space_invaders.bin\n",
      "  Cart Name: Space Invaders (1978) (Atari) [!]\n",
      "  Cart MD5:  72ffbef6504b75e69ee1045af9075f66\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        4096\n",
      "  Bankswitch Type: AUTO-DETECT ==> 4K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1669302901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (0.19.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (2.22.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (9.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (2022.10.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (1.23.4)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages (from packaging>=20.0->scikit-image) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "#instalación de librerías.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "!pip install gym[atari]==0.25.0\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install imageio\n",
    "!pip install matplotlib\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "from ale_py.roms import SpaceInvaders\n",
    "ale.loadROM(SpaceInvaders)\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "!pip install scikit-image\n",
    "from skimage import transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-26T10:23:58.642164Z",
     "iopub.status.idle": "2022-10-26T10:23:58.642528Z",
     "shell.execute_reply": "2022-10-26T10:23:58.642375Z",
     "shell.execute_reply.started": "2022-10-26T10:23:58.642358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La versión de gym instala: 0.25.0\n",
      "El entorno utiliza:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Comprobación de la versión de GYM instalada\n",
    "print('La versión de gym instala: ' + gym.__version__)\n",
    "# Comprobación de entorno con gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"El entorno utiliza: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comprueba que la celda anterior indica que la verisón instalada de Gym es la 0.25.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listamos los dispositivos CUDA disponibles con sus propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0: NVIDIA GeForce RTX 3070\n",
      "Properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3070', major=8, minor=6, total_memory=8191MB, multi_processor_count=46)\n",
      "\n",
      "Device #1: NVIDIA GeForce RTX 2060 SUPER\n",
      "Properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2060 SUPER', major=7, minor=5, total_memory=8191MB, multi_processor_count=34)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()): \n",
    "    print(f'Device #{i}: {torch.cuda.get_device_name(i)}')\n",
    "    print(f'Properties: {torch.cuda.get_device_properties(i)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfo8jleHGadK"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.1 (0.2 ptos):</strong> Inicializar el entorno 'SpaceInvaders-v4'. Extraer:\n",
    "<ul>\n",
    " <li> Valor del umbral de recompensa definido en el entorno</li>\n",
    " <li> Máximo número de pasos establecidos para cada episodio</li>\n",
    " <li> La dimensión del espacio de acciones</li>\n",
    " <li> La dimensión del espacio de observaciones.</li>\n",
    " </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "env = gym.make('SpaceInvaders-v4')\n",
    "\n",
    "##TODO:\n",
    "####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el aprendizaje por refuerzo suele ser muy útil visualizar el comportamiento de un agente en su entorno. Para esta PEC es interesante poder almacenar el comportamiento visual de un agente en forma de archivo .gif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.2 (0.2 ptos):</strong>  En este ejericicio os proponemos almacenar en una carpeta videos un ejemplo de actuación del agente aleatorio en forma de archivo .gif.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para generar la imagen a partir de un estado con un texto informativo.\n",
    "def _label_with_text(frame):\n",
    "    '''\n",
    "    frame: estado de un entorno GYM.\n",
    "    '''\n",
    "    im = Image.fromarray(frame)\n",
    "    im = im.resize((im.size[0]*2,im.size[1]*2))\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    drawer.text((1, 1), \"Uoc Aprendizaje Por Refuerzo.\", fill=(255, 255, 255, 128))\n",
    "    return im\n",
    "\n",
    "#Método que permite crear un gif con la evolución de una partida dado un entorno GYM.\n",
    "def save_random_agent_gif(env):\n",
    "    frames = []\n",
    "    done = False\n",
    "    env.reset()\n",
    "    ###########################################\n",
    "    #TODO Jugar una partida aleatoria.\n",
    "    while not done:\n",
    "        action = None\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(_label_with_text(frame))\n",
    "        state, _, done, _ = None\n",
    "    ##############################################\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimwrite(os.path.join('./videos/', 'random_agent_space_invader_usuario.gif'), frames, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/geviar/anaconda3/envs/rl/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_random_agent_gif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m save_random_agent_gif(env)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_random_agent_gif' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "try:\n",
    "    os.makedirs('videos')\n",
    "except:\n",
    "    pass\n",
    "save_random_agent_gif(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 1.3 (0.3 ptos):</strong> Ejecutar 1000 episodios con el máximo de pasos establecido en el entorno de Space Invader, tomando acciones de forma aleatoria. Almacenar la suma de recompensas de cada partida y la cantidad de episodios ejecutados. Mostrar:\n",
    "    <ul>\n",
    "       <li>Histograma con la suma de recompensas de cada partida</li>\n",
    "       <li>Histograma con la cantidad de pasos para resolver cada partida. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis (0.3 ptos):</strong> ¿Cuál es la media de recompensas obtenida? ¿Y la media de pasos por episodio? Comenta los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agente DQN (2.8 ptos)\n",
    "En este apartado implementaremos una solución DQN para intentar obtener un modelo que nos permita solucionar este entorno. Primeramente definiremos el modelo de red neuronal, luego describiremos el comportamiento del agente, lo entrenaremos y, finalmente, testearemos el funcionamiento del agente entrenado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Procesamiento de las observaciones.\n",
    "\n",
    "\n",
    "El primer paso es entender la estructura de la información que nos proporciona el entorno. Los juegos de Atari utilizan un espacio de acción con una estructura de la siguiente forma (210, 160, 3), es decir, 210 pixeles de anchura, 160 de altura y 3 colores (RGB). Cada uno de estos puntos de la estructura es un pixel de color y posee un rango de valores que van desde el 0 hasta 255, lo que nos da $256^{(210x160x3)}$ = $256^{100800}$ posibilidades (a modo de comparación, tenemos aproximadamente $10^{80}$ átomos en el universo observable)\n",
    "\n",
    "<p></p>\n",
    "<img src=\"imagenes/atomos.jpg\"  width=\"1000\">\n",
    "\n",
    "Fuente: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.1 Análisis (0.2 pto):</strong> ¿Es posible implementar una solución tabular vista durante la PEC1 en este entorno de Space Invader?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Respuesta:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, la mejor idea es, dado un estado, aproximar los valores Q para cada posible acción en ese estado. Es decir, implementaremos la solución que puedes observar en la siguiente figura.\n",
    "\n",
    "<p></p>\n",
    "<img src=\"imagenes/deep.jpg\"  width=\"1000\">\n",
    "\n",
    "Fuente: <href>https://huggingface.co/blog/deep-rl-dqn</href>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Ahora bien, en el juego Space Invader no todo el espacio posee utilidad. El marcador, el marco o la cantidad de vidas no es necesario para el desarrollo del juego. Una práctica muy habitual en el aprendizaje por refuerzo es reducir las dimensiones del entorno con el objetivo de disminuir la cantidad de cálculos necesarios para obtener un modelo útil para el entorno.\n",
    "\n",
    "Exactamente, para esta PEC os vamos a pedir reducir el espacio de los frames a 84 x 84 píxeles, reducir nuestros tres canales de color (RGB) a 1 (blanco y negro), normalizar el resultado y, por último, almacenar los 4 últimos frames cómo se muestra en la siguiente figura:\n",
    "<p></p>\n",
    "<img src=\"imagenes/preprocessing.jpg\"  width=\"1000\">\n",
    "\n",
    "Fuente: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.2  (0.1 pto):</strong> Define el metodo scale_lumininance para, dado un estado, transformar los puntos del sistema RGB al sistema binario blanco y negro\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_lumininance(obs):\n",
    "    ####### TODO #######\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.3 (0.1 pto):</strong> Define el metodo resize para, dado un estado, redimensionar su tamaño a 84 * 84\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(obs):\n",
    "    ####### TODO #######\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.4 Ejercicio (0.1 pto):</strong> Define el metodo normalize para, dado un estado, normalice la imagen\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(obs):\n",
    "    ####### TODO #######\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado que vas a obtener será el siguiente:\n",
    "<p></p>\n",
    "<table  border=\"0\">\n",
    "<thead>\n",
    "  <tr style='border:none;' >\n",
    "    <th style='border:none;' ><img src=\"imagenes/preprocess_grey.png\"  width=\"500\"></th>\n",
    "    <th style='border:none;'><img src=\"imagenes/preprocess_normalize.png\"  width=\"500\"></th>\n",
    "    <th style='border:none;' ><img src=\"imagenes/preprocess_size.png\"  width=\"500\"></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr style='border:none;'>\n",
    "    <td style='text-align: center; border:none;'> 1.scale_lumininance(obs) </td>\n",
    "    <td style='text-align: center; border:none;'> 2.resize(obs) </td>\n",
    "    <td style='text-align: center; border:none;'> 3.normalize(obs)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='rgb_array')\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, _ = env.step(action)\n",
    "\n",
    "# Función que realiza todo el pre-procesado de una observación\n",
    "def preprocess_observation(obs):\n",
    "    obs_proc = scale_lumininance(obs)\n",
    "    obs_proc = resize(obs_proc)\n",
    "    obs_proc = normalize(obs_proc)\n",
    "    return obs_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, ¿cómo interpretamos el movimiento? Las imágenes corresponden a información estática de las partidas y, en este entorno, es importante conocer la dirección del disparo o el movimiento de los invasores. Para ello, una forma de gestionar esta información es apilando fotogramas para poder proporcionar al algoritmo información acerca de la progresión de la partida.\n",
    "\n",
    "\n",
    "Utilicemos un ejemplo muy claro, el Ping Pong:\n",
    "<p></p>\n",
    "<img src=\"imagenes/temporal-limitation-2.png\"  width=\"1000\">\n",
    "\n",
    "Fíjate cómo la pelota se desplaza hacia la derecha. La agrupación de los frames nos permite trasladar la información espacial a nuestro algoritmo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.5  (0.1 ptos):</strong> Implementa la función stack_frame para apilar 4 frames de una partida. Dicha función debe apilar inicialmente (cuando is_new = True) el mismo frame 4  veces para, posteriormente, conforme se vayan introduciendo nuevos frames ir substituyendo a los más antiguos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stack_frame(stacked_frames, frame, is_new):\n",
    "        \"\"\"Stacking Frames.\n",
    "        Params\n",
    "        ======\n",
    "            stacked_frames (array): array de frames (al devolverlo debe tener 4 frames)\n",
    "            frame: Nueva imagen a añadir (hay que borrar la más antigua)\n",
    "            is_new: Primera vez que se utiliza el array.\n",
    "        \"\"\"\n",
    "        \n",
    "        #### TODO ####\n",
    "        \n",
    "        return stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la función la testeamos jugando una partida, almacenando los 4 frames más recientes en cada paso, y finalmente mostramos los 4 últimos frames de la partida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenamos 'state_stack' con 4 copias idénticas del frame inicial de la partida\n",
    "state_stack= stack_frame(None, env.reset(), True)\n",
    "\n",
    "# Jugamos una partida aleatoria y vamos actualizando los 4 frames de 'state_stack'\n",
    "# conforme se van produciendo nuevos\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state_stack = stack_frame(state_stack, next_state, False)\n",
    "\n",
    "    if done:\n",
    "         break\n",
    "\n",
    "# Mostramos los últimos 4 frames de la partida que han quedado almacenados en 'state_stack'\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "rows = 2\n",
    "columns = 2\n",
    "cont=1\n",
    "\n",
    "for i in state_stack:\n",
    "    fig.add_subplot(rows, columns, cont)\n",
    "    plt.imshow(i)\n",
    "    plt.axis('off')\n",
    "    plt.title('Fotograma:' + str(cont))\n",
    "    cont = cont +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con estas modificaciones podemos pasar al siguiente punto, la creación de la Red\n",
    "<p></p>\n",
    "<img src=\"imagenes/deep-q-network.jpg\"  width=\"1000\">\n",
    "\n",
    "Fuente: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se observa en la imagen, utilizaremos un modelo convolucional (perfecto para detectar patrones en imágenes) junto con un sistema lineal completamente conectado. Para la red convolucional os proponemos la siguiente configuración:\n",
    "\n",
    "<ul>\n",
    "    <li>Una primera capa del tipo Conv2d con los siguientes parámetros in_channels=(8,64,64), out_channels=32, kernel_size=8, stride=4 con activación ReLU.</li>\n",
    "    <li>Una segunda capa del tipo Conv2d con los siguientes parámetros in_channels=32, out_channels=64, kernel_size=4, stride=2 con activación ReLU.</li>\n",
    "    <li>Una tercera capa del tipo Conv2d con los siguientes parámetros in_channels=64, out_channels=64, kernel_size=3, stride=1 con activación ReLU.</li>\n",
    "</ul>\n",
    "\n",
    "El resultado de esta primera red (3D) lo conectaremos con un modelo complementamente conectado (1D) con la siguiente estructura lineal:\n",
    "\n",
    "* Una primera capa completamente conectada (representada en pytorch por nn.Lineal) de 512 neuronas, con activación ReLU\n",
    "* Una última capa completamente conectada. Esta será nuestra capa de salida y, por lo tanto, tendrá tantas neuronas como dimensiones tenga nuestro espacio de acciones (una salida por cada acción posible).\n",
    "\n",
    "Por último, usaremos el optimizador Adam para entrenar la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 2.6 (0.5 ptos):</strong>Implementar la clase  <code>DQN_CNN()</code>. Inicializar las variables necesarias y definir el modelo CNN y linial indicado.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DQN_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        super(DQN_CNN, self).__init__()\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        device: cpu o cuda\n",
    "        red_cnn: definición de la red convolucional\n",
    "        red_lineal: definición de la red lineal\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ###TODO: inicialización y modelo###\n",
    "        self.input_shape =None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal convolucional\n",
    "        self.red_cnn =None\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_cnn.cuda()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal lineal completamente conectada\n",
    "        self.red_lineal = None\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_lineal.cuda()\n",
    "\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = None\n",
    "\n",
    "    ### e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        cnn_out = self.red_cnn(state_t).reshape(-1,  self.fc_layer_inputs)\n",
    "        return self.red_lineal(cnn_out)\n",
    "\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable(torch.zeros(1, *self.input_shape)).to(device=self.device)).view(1, -1).size(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase para el *buffer* de repetición de experiencias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class experienceReplayBuffer:\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use el operador asterisco para desempaquetar deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Definición del agente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación implementaremos una clase que defina el entrenamiento del agente teniendo en cuenta:\n",
    "    <ul>\n",
    "        <li>La exploración/explotación (decaimiento de epsilon)</li>\n",
    "        <li>La actualización y sincronización de la red principal y la red objetivo (pérdida)</li>\n",
    "    </ul>\n",
    "\n",
    "Además, vamos a considerar que el agente ha superado el entorno cuando obtenga una puntuación superior a 350 en 100 partidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.7 (1 pto):</strong> Implementar los siguientes puntos de la clase <code>DQNAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar las variables de la clase</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Actualizar la red principal según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la ecuación de Bellman</li>\n",
    "        <li>Sincronizar la red objetivo según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "        <li>Actualizar epsilon según: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01)$$ </li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias de los 100 episodios anteriores</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "class DQNAgent:\n",
    "    ###################################################\n",
    "    ######TODO 1: declarar variables ##################\n",
    "    def __init__(self, env, main_network,\n",
    "                 buffer, reward_threshold,\n",
    "                 epsilon=0.1, eps_decay=0.99, batch_size=32, nblock):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        main_network: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = None\n",
    "        self.reward_threshold = None\n",
    "        self.initialize()\n",
    "\n",
    "    ###############################################################\n",
    "    #####TODO 2: inicialitzar variables extra que es necesiten######\n",
    "    def initialize(self):\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        incio_juego = preprocess_observation(self.env.reset())\n",
    "        self.state0 = stack_frame(None, incio_juego, True)s\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO 3:  Tomar nueva acción ###############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()  # acción aleatoria en el burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, eps)# acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realización de la acción y obtención del nuevo estado y la recompensa.\n",
    "        new_state, reward, done, _ = None \n",
    "        new_state =  None #Recordar preprocesar los estados\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardar experiencia en el buffer\n",
    "        self.state0 = new_state\n",
    "\n",
    "        #TODO: resetear entorno 'if done'\n",
    "        if done:\n",
    "            self.state0 =None #Recordar preprocesar los estados\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250, min_epsilon = 0.01):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = 0\n",
    "        while training:\n",
    "            self.state0 = None #Recordar preprocesar los estados\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                ###TODO 4: Actualizar red principal según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Almacenar epsilon, training rewards i loss#######\n",
    "\n",
    "                    ##################################################################\n",
    "                    \n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = None\n",
    "                    \n",
    "                    ##################################################################\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {} , Maximo {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon,maximo), end=\"\")\n",
    "\n",
    "                    # Comprobar si se ha llegado al máximo de episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    # y se ha entrenado un mínimo de episodios\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualizar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "\n",
    "    ####################################\n",
    "    #####TODO 5: Cálculo de la pérdida ####\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).to(device=self.device).reshape(-1,1)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenemos los valores de Q de la red objetivo El parametro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "\n",
    "        #################################################################################\n",
    "        ###TODO: Calculamos ecuación de Bellman\n",
    "        expected_qvals =None\n",
    "\n",
    "\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch)# calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entrenamiento\n",
    "A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.001 </li>\n",
    "        <li>Tamaño del batch: 32</li>\n",
    "        <li>Capacidad máxima del buffer: 8000</li>\n",
    "        <li>Gamma: 0.99</li>\n",
    "        <li>Epsilon: 1, con decaimiento de 0.995 con un mínimo de 0.01</li>\n",
    "        <li>Número de steps para rellenar el buffer: 100</li>\n",
    "        <li>Número máximo de episodios: 5000</li>\n",
    "        <li>Número minímo de episodios: 250</li>\n",
    "        <li>Frecuencia de actualización de la red neuronal: 100 </li>\n",
    "        <li>Frecuencia de sincronización con la red objetivo: 5000</li>\n",
    "    </ul>\n",
    "\n",
    "Es probable que con los hiperparámetros propuestos no se llegue a solucionar el entorno (conseguir una puntuación de 350 en la media de 100 partidas). El objetivo principal es mejorar los resultados del agente aleatorio.\n",
    "\n",
    "Está permitido realizar modificaciones en los parámetros presentados aunque, el último ejercicio de esta PEC, consiste en mejorar los resultados obtenidos con este entrenamiento.  Por lo tanto, aunque esté permitido, no es recomendable.\n",
    "\n",
    "Si el alumno no dispone de suficiente tiempo puede acortar el tiempo de entrenamiento fijando el Número máximo de episodios = 3000 en lugar de los 5000 propuestos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.8 (0.1 ptos):</strong> Declarar los hiperparámetros, cargar el modelo de red neuronal y entrenar el agente\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.9 (0.1 ptos):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gráfico con las recompensas obtenidas a lo largo del entrenamieno, la evolución de las recompensas medias y el umbral de recompensa establecido por el entorno.</li>\n",
    "        <li>Gráfico con la evolución de la perdida a lo largo del entrenamiento</li>\n",
    "        <li>Gráfico con la evolución de epsilon a lo largo del entrenamiento</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 2.10 (0.1 ptos):</strong> Comenta los resultados\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución:</strong>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 2.11 (0.1 pto):</strong> Guardar el modelo entrenado en formato \".pth\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test del agente entrenado\n",
    "\n",
    "Una vez entrenado el agente, nos interesa comprobar cómo de bien ha aprendido, si el \"robot\" es capaz de realizar las tareas aprendidas. Para ello, recuperamos el modelo entrenado y dejamos que el agente tome acciones aleatorias según ese modelo y observamos su comportamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 2.12 (0.2 pto):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 20 episodios consecutivos. Calcular la suma de recompensas para cada episodio. Mostrar en un gráfico la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido por el entorno.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDQN.main_network.load_state_dict(torch.load(\"agentDQN_Trained_Model_dqn_cnn.pth\"))\n",
    "##TODO: Calcular la suma de recompensas\n",
    "\n",
    "##TODO: realizar las gráficas que se piden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 2.13 (0.1 pto):</strong> Almacena una partida de ejemplo del agente en la carpeta videos en formato GIF para poder visualizar su comportamiento (se da el código hecho) y comenta el comportamiento del agente entrenado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "def save_agent_gif(env, ag, nombre_fichero):\n",
    "    '''\n",
    "    :param env:  entorno GYM\n",
    "    :param ag:  agente entrenado\n",
    "    :param nombre_fichero:  nombre del fichero\n",
    "    :return:\n",
    "    '''\n",
    "    frames = []\n",
    "    env.reset()\n",
    "    observation = env.reset()\n",
    "    incio_juego = preprocess_observation(observation)\n",
    "    state= stack_frame(None, incio_juego, True)\n",
    "    total_reward = 0\n",
    "    t=0\n",
    "    while True:\n",
    "            state= stack_frame(state, preprocess_observation(observation), False)\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            frames.append(_label_with_text(frame))\n",
    "            action = ag.main_network.get_action(state,epsilon=0.0)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            t=t+1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    try:\n",
    "        os.makedirs('videos')\n",
    "    except:\n",
    "        pass\n",
    "    imageio.mimwrite(os.path.join('./videos/', nombre_fichero), frames, fps=60)\n",
    "save_agent_gif(env,agentDQN, 'agente_dqn.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Respuesta:</strong>\n",
    "<br><br>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ser que el resultado sea muy parecido al siguiente GIF. No os preocupéis, con los parámetros escogidos es difícil obtener unos mejores resultados.\n",
    "![title](videos/space_invader_dqn_comportamiento_continuo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agente Dueling DQN (2.2 ptos)\n",
    "\n",
    "En este apartado resolveremos el mismo entorno con las mismas características para el agente, pero usando una dueling DQN. Como en el caso anterior, primero definiremos el modelo de red neuronal, luego describiremos el comportamiento del agente, lo entrenaremos y, finalmente, testearemos el funcionamiento del mismo.\n",
    "\n",
    "### 3.1 Definición de la arquitectura  de la red neuronal\n",
    "\n",
    "\n",
    "El objetivo principal de las dueling DQN es \"ahorrarse\" el cálculo del valor de Q en aquéllos estados en los que es irrelevante la acción que se tome. Para ello se descompone la función Q en dos componentes:\n",
    "\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V (s)$$\n",
    "\n",
    "\n",
    "Esta descomposición se realiza a nivel de la arquitectura de la red neuronal. Las primeras capas que teníamos en la DQN serán comunes, y luego la red se dividirá en dos partes separadas definidas por el resto de capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descomposición en sub-redes del modelo de la DQN implementada en el apartado anterior, será entonces:\n",
    "<ol>\n",
    "    <li>Bloque común, la red CNN</li>\n",
    "    <li>Red advantage A(s,a):</li>\n",
    "         <ul>\n",
    "             <li>Una primera capa completamente conectada (representada en pytorch por nn.Lineal) de 512 neuronas, con activación ReLU</li>\n",
    "             <li>Una última capa completamente conectada. Esta será nuestra capa de salida y por lo tanto el número de neuronas de salida dependerá del tipo de red, A(s,a) en este caso, y tendrá tantas neuronas como dimensiones tenga el espacio de acciones.</li>\n",
    "             </ul>\n",
    "    <li>Red value V(s):</li>\n",
    "            <ul>\n",
    "             <li>Una primera capa completamente conectada (representada en pytorch por nn.Lineal) de 512 neuronas, con activación ReLU</li>\n",
    "             <li>Una última capa completamente conectada. Esta será nuestra capa de salida con un valor por estado.</li>\n",
    "             </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.1 (0.75 ptos):</strong> Implementar la clase <code>duelingDQN()</code>. Inicializar las variables necesarias y definir el modelo Funcional de red neuronal indicado.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class duelingDQN(torch.nn.Module):\n",
    "    ###################################\n",
    "    ###TODO: inicialización y modelo###\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        input_shape: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        device: cpu o cuda\n",
    "        red_cnn: definición de la red convolucional\n",
    "        value: definición de la red lineal value\n",
    "        advantage: definición de la red lineal advantage\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        ####TODO: Inicializar variables####\n",
    "        super(duelingDQN, self).__init__()\n",
    "        self.input_shape = None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal\n",
    "        self.red_cnn = None\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_cnn.cuda()\n",
    "        self.advantage = None\n",
    "        self.value =None\n",
    "\n",
    "        ### Se ofrece la opción de trabajar con cuda\n",
    "        if self.device == 'cuda':\n",
    "            self.value.cuda()\n",
    "            self.advantage.cuda()\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "        \n",
    "        ### Inicializamos el optimizador\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    #######################################\n",
    "    #####TODO: función forward#############\n",
    "    def forward(self, x):\n",
    "        return\n",
    "\n",
    "    ### Método e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acción aleatoria\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acción a partir del cálculo del valor de Q para esa acción\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.forward(state_t)\n",
    "\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable( torch.zeros(1, * self.input_shape)).to(device=self.device)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el buffer de repetición de experiencias podemos usar exactamente la misma clase experienceReplayBuffer descrita en el apartado anterior de la DQN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Definición del agente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre la DQN y la dueling DQN se centra, como hemos visto, en la definición de la arquitectura de la red. Pero el proceso de aprendizaje y actualización es exactamente el mismo. Así, podemos recuperar la clase implementada en el apartado anterior, DQNAgent() y reutilizarla aquí bajo el nombre de duelingDQNAgent(). Lo único que deberemos hacer es añadir el optimizador entre las variables a declarar y adaptar la función de pérdida al formato Functional de pytorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.2 (0.5 pto):</strong> Implementar la clase <code>duelingDQNAgent()</code> como la <code>DQNAgent()</code>\n",
    "<p>\n",
    "</p>\n",
    "De nuevo, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias de los 100 episodios anteriores</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    " ###################################################\n",
    "    ######TODO 1: declarar variables ##################\n",
    "    def __init__(self, env, main_network,\n",
    "                 buffer, reward_threshold,\n",
    "                 epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = None\n",
    "        self.reward_threshold = None\n",
    "        self.initialize()\n",
    "\n",
    "    ###############################################################\n",
    "    #####TODO 2: inicialitzar variables extra que es necessiten######\n",
    "    def initialize(self):\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = None\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO 3:  Tomar nueva acción ###############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()  # acción aleatoria en el burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, eps)# acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realización de la acción y obtención del nuevo estado y la recompensa\n",
    "        new_state, reward, done, _ = None  #\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardar experiencia en el buffer\n",
    "        self.state0 = new_state.copy()\n",
    "\n",
    "        #TODO: resetear entorno 'if done'\n",
    "        if done:\n",
    "            incio_juego = preprocess_observation(self.env.reset())\n",
    "            self.state0 =None\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250, min_epsilon = 0.01):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = 0\n",
    "        while training:\n",
    "            self.state0 = None\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                #####TODO 4: Actualizar la red principal según la frecuencia establecida  #######\n",
    "\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Almacenar epsilon, training rewards i loss#######\n",
    "\n",
    "                    ####\n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = None\n",
    "                    ###\n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {} , Maximo {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon,maximo), end=\"\")\n",
    "\n",
    "                    # Comprobar si se ha llegado al máximo de episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    # y se ha entrenado un mínimo de episodios\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualizar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "\n",
    "     ## Cálculo de la pérdida\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device).reshape(-1,1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "\n",
    "        #DQN update#\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states), dim=-1)[1]\n",
    "        if self.device == 'cuda':\n",
    "            next_actions_vals = next_actions.reshape(-1,1).to(\n",
    "                device=self.device)\n",
    "        else:\n",
    "            next_actions = torch.max(self.dnnetwork.get_qvals(next_states), dim=-1)[1]\n",
    "        # Obtenemos los valores de Q de la red objetivo\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals).detach()\n",
    "        #####\n",
    "\n",
    "        qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "\n",
    "        #TODO: Calculamos ecuación de Bellman\n",
    "        expected_qvals = None\n",
    "        \n",
    "        # Modificar la función de Loss para el modo Functional######\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        #######\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Entrenamiento\n",
    "A continuación entrenaremos el modelo dueling DQN con los mismos hiperparámetros con los que entrenamos la DQN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.3 (0.1 ptos):</strong> Cargar el modelo de red neuronal y entrenar el agente con los mismos hiperparámetros usados para la DQN\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.4 (0.2 ptos):</strong> Mostrar los mismos gráficos que con la DQN:\n",
    "    <ol>\n",
    "        <li>Recompensas obtenidas a lo largo del entrenamieno y la evolución de las recompensas medias cada 100 episodios, junto con el umbral de recompensa establecido por el entorno</li>\n",
    "        <li>Pérdida durante el entrenamiento</li>\n",
    "        <li>Evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 3.5 (0.1 pto):</strong> Guardar el modelo entrenado en formato \".pth\" y comentar los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 3.6 (0.1 pto):</strong> Comenta los resultados obtenidos\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test del agente\n",
    "Finalmente analizamos el comportamiento del agente entrenado.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.7 (0.2 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 20 episodios consecutivos. Calcular la suma de recompensas por cada episodio. Mostrar en un gráfico la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido por el entorno.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 3.8 (0.25 ptos):</strong> Almacena una partida de ejemplo del agente en la carpeta videos en formato GIF para poder visualizar su comportamiento y comenta el comportamiento del agente entrenado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "save_agent_gif(env,duelingDQNAgent , 'space_invader_duelingDQNAgent_comportamiento.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. REINFORCE with baseline (2 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definición de la arquitectura  de la red neuronal\n",
    "Utilizaremos un modelo Secuencial con la siguiente configuración:\n",
    "\n",
    "<ul>\n",
    "    <li>Una primera capa del tipo Conv2d con los siguientes parámetros in_channels=(8,64,64), out_channels=32, kernel_size=8, stride=4 con activación ReLU.</li>\n",
    "    <li>Una segunda capa del tipo Conv2d con los siguientes parámetros in_channels=32, out_channels=64, kernel_size=4, stride=2 con activación ReLU.</li>\n",
    "    <li>Una tercera capa del tipo Conv2d con los siguientes parámetros in_channels=64, out_channels=64, kernel_size=3, stride=1 con activación ReLU.</li>\n",
    "</ul>\n",
    "\n",
    "El resultado de esta primera red lo conectaremos con un modelo complementamente conectado con la siguiente estructura lienal:\n",
    "\n",
    "* Una primera capa completamente conectada (representada en pytorch por nn.Lineal) de 512 neuronas, bias=True y  con activación Tanh\n",
    "* Una última capa completamente conectada. Esta será nuestra capa de salida y, por lo tanto, tendrá tantas neuronas como dimensiones tenga nuestro espacio de acciones (una salida por cada acción posible), bias=True y activación Softmax (dim=-1).\n",
    "\n",
    "Por último, usaremos el optimizador Adam para entrenar la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1 (0.5 ptos):</strong> Implementar la clase <code>PGReinforce()</code>. Inicializar las variables necesarias y definir el modelo Secuencial de red neuronal indicado.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGReinforce(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "        super(PGReinforce, self).__init__()\n",
    "         ###################################\n",
    "        ####TODO: Inicializar variables####\n",
    "        self.input_shape =None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        self.device = 'cpu'\n",
    "        self.learning_rate = learning_rate\n",
    "        ######\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal\n",
    "        self.red_cnn = None\n",
    "        if self.device == 'cuda':\n",
    "            self.red_cnn.cuda()\n",
    "\n",
    "\n",
    "        self.red_lineal = None\n",
    "\n",
    "        ### Se ofrece la opción de trabajar con cuda\n",
    "        if self.device == 'cuda':\n",
    "            self.red_lineal.cuda()\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        #####\n",
    "\n",
    "    #Obtención de las probabilidades de las posibles acciones\n",
    "    def get_action_prob(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        cnn_out = self.red_cnn(state_t).reshape(-1,  self.fc_layer_inputs)\n",
    "        return self.red_lineal(cnn_out)\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable( torch.zeros(1, * self.input_shape)).to(device=self.device)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Definición del agente\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2 (0.55 pto):</strong> Implementar los siguientes puntos de la clase <code>reinforceAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar las variables de la clase</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Calcular el <i>discounted reward</i> usando como línea de base la media estandardizada del retorno\n",
    "               $$ \\frac{x_i - \\bar{x}}{\\sigma_x}$$</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "        <li>Implementar la pérdida por actualización</li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias de los 100 episodios anteriores</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforceAgent:\n",
    "\n",
    "    ###################################################\n",
    "    ######TODO 1: declarar variables ##################\n",
    "    def __init__(self, env, dnnetwork):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.dnnetwork = None\n",
    "        self.nblock = None\n",
    "        self.reward_threshold =None\n",
    "        self.initialize()\n",
    "     #######\n",
    "\n",
    "    ###############################################################\n",
    "    #####TODO 2: inicializar variables extra que se necesiten######:\n",
    "    def initialize(self):\n",
    "       self.batch_rewards = []\n",
    "       self.batch_actions = []\n",
    "       self.batch_states = []\n",
    "       self.batch_counter = 1\n",
    "    ######\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=2000, batch_size=10):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        episode = 0\n",
    "        action_space = np.arange(self.env.action_space.n)\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            incio_juego = preprocess_observation(self.env.reset())\n",
    "            state0 = stack_frame(None, incio_juego, True)\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            episode_actions = []\n",
    "            gamedone = False\n",
    "\n",
    "            while gamedone == False:\n",
    "                ##########################################################\n",
    "                ######TODO 3:  Tomar nueva acción ########################\n",
    "                action_probs = None  #distribución de probabilidad de las acciones dado el estado actual\n",
    "                action = None   #acción aleatoria de la distribución de probabilidad\n",
    "                next_state, reward, gamedone, _ = None\n",
    "                #######\n",
    "\n",
    "\n",
    "                # Almacenamos experiencias que se van obteniendo en este episodio\n",
    "                episode_states.append(state0)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                next_state = stack_frame(state0, preprocess_observation(next_state), False)\n",
    "                state0 = next_state\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    # Calculamos el término del retorno menos la línea de base\n",
    "                    self.batch_rewards.extend(self.discount_rewards(episode_rewards))\n",
    "                    self.batch_states.extend(episode_states)\n",
    "                    self.batch_actions.extend(episode_actions)\n",
    "                    self.batch_counter += 1\n",
    "\n",
    "                    #####################################################################################\n",
    "                    ###TODO 5: calcular media de recompensas de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = None\n",
    "                    ######\n",
    "\n",
    "\n",
    "                    # Actualizamos la red cuando se completa el tamaño del batch\n",
    "                    if self.batch_counter == self.batch_size:\n",
    "                        self.update(self.batch_states, self.batch_rewards, self.batch_actions)\n",
    "\n",
    "                        ####################################\n",
    "                        ###TODO : almacenar training_loss###\n",
    "                        ###########\n",
    "\n",
    "                        self.update_loss = []\n",
    "\n",
    "                        # Reseteamos las variables del epsiodio\n",
    "                        self.batch_rewards = []\n",
    "                        self.batch_actions = []\n",
    "                        self.batch_states = []\n",
    "                        self.batch_counter = 1\n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards), end=\"\")\n",
    "\n",
    "                    # Comprobamos que todavía quedan episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "    ########################################################\n",
    "    ###TODO 4: cálculo del retorno menos la línea de base###\n",
    "    def discount_rewards(self, rewards):\n",
    "        discount_r = np.zeros_like(rewards)\n",
    "        timesteps = range(len(rewards))\n",
    "        reward_sum = 0\n",
    "        for i in reversed(timesteps):\n",
    "            reward_sum = rewards[i] + self.gamma*reward_sum\n",
    "            discount_r[i] = reward_sum\n",
    "        baseline = None\n",
    "        return baseline\n",
    "    ##########\n",
    "\n",
    "\n",
    "    ## Actualización\n",
    "    def update(self, batch_s, batch_r, batch_a):\n",
    "        self.dnnetwork.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        state_t = torch.FloatTensor(batch_s)\n",
    "        reward_t = torch.FloatTensor(batch_r)\n",
    "        action_t = torch.LongTensor(batch_a)\n",
    "        loss = self.calculate_loss(state_t, action_t, reward_t) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.dnnetwork.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "    #################################################\n",
    "    ###TODO 6: Cálculo de la pérdida#################\n",
    "    # Recordatorio: cada actualización es proporcional al producto del retorno y el gradiente de la probabilidad\n",
    "    # de tomar la acción tomada, dividido por la probabilidad de tomar esa acción (logaritmo natural)\n",
    "    def calculate_loss(self, state_t, action_t, reward_t):\n",
    "        logprob = None\n",
    "        selected_logprobs = None\n",
    "        loss = None\n",
    "        return loss\n",
    "     ########\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrenamiento\n",
    "A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.005</li>\n",
    "        <li>Tamaño del batch: 8</li>\n",
    "        <li>Gamma: 0.99</li>\n",
    "        <li>Número máximo de episodios: 5000</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.3 (0.2 ptos):</strong> Definir los hiperparámetros, cargar el modelo de red neuronal y entrenar el agente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.4 (0.1 ptos):</strong> Mostrar los siguientes gráficos:\n",
    "    <ol>\n",
    "        <li>Recompensas obtenidas a lo largo del entrenamieno y la evolución de las recompensas medias cada 100 episodios, junto con el umbral de recompensa establecido por el entorno</li>\n",
    "        <li>Pérdida durante el entrenamiento</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 4.5  (0.2 ptos):</strong> Guardar el modelo entrenado en formato \".pth\" y comentar los resultados obtenidos. ¿Qué ha ocurrido con este modelo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.6 (0.15 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 20 episodios consecutivos. Calcular la suma de recompensas por cada episodio. Mostrar en un gráfico la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido por el entorno. Comentar los resultados.\n",
    "<p>Almacena una partida de ejemplo del agente en la carpeta videos en formato GIF para poder visualizar su comportamiento y coméntalo.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación de modelos (1 pto)\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 5.1 (0.5 ptos):</strong> Muestra en un mismo gráfico la evolución de la media de recompensas de los tres modelos, junto con el umbral de recompensa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis 5.2 (0.5 ptos):</strong> Analizar los resultados obtenidos teniendo en cuenta el número de episodios, el tiempo de entrenamiento y el rendimiento de los tres agentes entrenados observado en apartados anteriores. ¿Qué agente presenta un mejor comportamiento? ¿Por qué?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución (Comentarios):</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimización (1 pto)\n",
    "En este apartado queremos encontrar la mejor arquitectura e hiperparámetros para optimizar la precisión del modelo. Los puntos que queremos tener en cuenta para la búsqueda del mejor modelo son los siguientes:\n",
    "<ul>\n",
    "    <li>Número de unidades de las capas</li>\n",
    "    <li>Learning rate</li>\n",
    "    <li>Actualización de la red principal</li>\n",
    "    <li>Sincronización de la red objetivo</li>\n",
    "    <li>Batch size</li>\n",
    "    <li>Gamma</li>\n",
    "    <li>Número de partidas del entrenamiento</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis modificar o adaptar la configuración cómo consideréis oportuno siempre y cuando justifiquéis el beneficio y el cambio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 6.1 :</strong> Elegir uno de los dos modelos de DQN implementados (DQN o dueling DQN) y  experimentar con diferentes combinaciones de valores para los parámetros especificados. El objetivo es conseguir un modelo con mejores resultados que el presentado en la PEC. <b>Indicar</b> las pruebas realizadas, pero presentar <b>únicamente</b> la ejecución y resultados de la mejor opción. Justificar los valores elegidos de hiperparámetros para las distintas pruebas realizadas y comentar los resultados de entrenamiento y de rendimiento del agente entrenado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solución:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "space_invader.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "10d40e26a39c540065936c3e7b50303c02ea46af6b04425647c283cb61da0a0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
